{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Concatenate, Multiply, Permute, Lambda, Reshape, Activation, Dropout, Add, GlobalMaxPooling1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import pydot, graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all of the CSV file paths\n",
    "stator_csv_files = [r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\01_SCIM_current_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\02_SCIM_current_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\03_SCIM_current_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\04_SCIM_current_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\05_SCIM_current_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\06_SCIM_current_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\07_SCIM_current_PTPbc_400V.csv\"]\n",
    "\n",
    "rotor_csv_files = [r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\01_SCIM_vibration_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\02_SCIM_vibration_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\03_SCIM_vibration_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\04_SCIM_vibration_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\05_SCIM_vibration_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\06_SCIM_vibration_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Simulink_data_generation\\Final_current_vibration_data\\07_SCIM_vibration_PTPbc_400V.csv\"]\n",
    "\n",
    "# Read the CSV files into pandas dataframe and convert to numpy arrays\n",
    "stator_data = [pd.read_csv(file).values for file in stator_csv_files]\n",
    "rotor_data = [pd.read_csv(file).values for file in rotor_csv_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(stator_data, rotor_data, sequence_length=100, batch_size=32):\n",
    "    # get the number of samples\n",
    "    num_samples = stator_data[0].shape[0] - sequence_length     \n",
    "\n",
    "    # Infinite loop to continuously yield the batches\n",
    "    while True:\n",
    "        # Generate batches\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            stator_batch = []\n",
    "            rotor_batch = []\n",
    "            batch_y = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                end_idx = start_idx + i + sequence_length\n",
    "                if end_idx >= num_samples:\n",
    "                    break\n",
    "\n",
    "            # Create sequences of length 'Sequence length'\n",
    "            stator_seq = [data[start_idx + i:end_idx, 1:] for data in stator_data] # excluding the time column\n",
    "            rotor_seq = [data[start_idx + i:end_idx, 1] for data in rotor_data] # excluding the time column & vibration is the only remaining column\n",
    "            \n",
    "            # Target is the next time step's 'Time' value in stator data (same as rotor data)\n",
    "            y = stator_data[0][end_idx, 0] # assuming time is the first column in the stator files\n",
    "\n",
    "            stator_batch.append(np.concatenate(stator_seq, axis=-1))\n",
    "            rotor_batch.append(np.concatenate(rotor_seq, axis=-1))\n",
    "            batch_y.append(y)\n",
    "        \n",
    "        if len(stator_batch) == batch_size:\n",
    "            X_stator = np.array(stator_batch)\n",
    "            X_rotor = np.array(rotor_batch)\n",
    "            y = np.array(batch_y)\n",
    "\n",
    "            #concatenating data along the last axis\n",
    "            yield [X_stator, X_rotor], y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generator\n",
    "\n",
    "generator = data_generator(stator_data, rotor_data, sequence_length=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BiLSTM network with Time and Channel attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time attention mechanism \n",
    "class TimeAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, **kwargs):\n",
    "        super(TimeAttentionLayer, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_omega = Dense(hidden_size, use_bias=False)\n",
    "        self.b_omega = Dense(hidden_size, use_bias=False)\n",
    "        self.u_omega = self.add_weight(shape=(hidden_size,), initializer='random_normal', trainable=True, name='u_omega')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_t = tf.nn.tanh(self.W_omega(inputs) + self.b_omega(inputs))\n",
    "        alpha_t = tf.nn.softmax(tf.reduce_sum(u_t * self.u_omega, axis=2, keepdims=True), axis=1)\n",
    "        context_vector = tf.reduce_sum(alpha_t * inputs, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "class ChannelAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ChannelAttentionLayer, self).__init__(**kwargs)\n",
    "        self.input_shape_param = input_shape\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.gap = GlobalAvgPool1D()\n",
    "        self.fcl = Dense(input_shape, activation='linear')\n",
    "        self.m = Dense(input_shape // 8, activation='relu')\n",
    "        self.v = Dense(input_shape // 8, activation='relu')\n",
    "        self.n = Dense(input_shape // 8, activation='relu')\n",
    "        self.W11 = Dense(input_shape // 8, activation='relu')\n",
    "        self.W12 = Dense(input_shape, activation='linear')\n",
    "        self.W21 = Dense(input_shape // 8, activation='relu')\n",
    "        self.W22 = Dense(input_shape, activation='linear')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ChannelAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_reshaped = tf.expand_dims(inputs, axis=1)\n",
    "\n",
    "        # GMP and GAP for descriptors m and v\n",
    "        gmp = self.gmp(inputs_reshaped)\n",
    "        gap = self.gap(inputs_reshaped)\n",
    "        \n",
    "        # Fully Connected Layer for descriptor n\n",
    "        fcl = self.fcl(inputs)\n",
    "\n",
    "        # Descriptors\n",
    "        m = self.m(gmp)\n",
    "        v = self.v(gap)\n",
    "        n = self.n(fcl)\n",
    "\n",
    "        # Multilayer Perceptrons\n",
    "        W11 = self.W11(v)\n",
    "        W12 = self.W12(W11)\n",
    "\n",
    "        W21 = self.W21(m)\n",
    "        W22 = self.W22(W21)\n",
    "\n",
    "        # Combine using element-wise summation and add n\n",
    "        combined = Add()([W12, W22, n])\n",
    "\n",
    "        # Activation function to get attention weights\n",
    "        attention_weights = Activation('hard_sigmoid')(combined)\n",
    "        attention_weights = Reshape((1, self.input_shape_param))(attention_weights)\n",
    "\n",
    "        # Apply attention weights\n",
    "        channel_attention = Multiply()([inputs, attention_weights])\n",
    "        return channel_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with modified channel attention mechanism and dropout\n",
    "def create_model(stator_input_shape, rotor_input_shape):\n",
    "    hidden_size = 64 # The size used for LSTM layers\n",
    "    # Stator current input\n",
    "    input_stator = Input(shape=stator_input_shape, name='stator_input')\n",
    "    lstm_stator = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_stator)\n",
    "    time_attention_stator = TimeAttentionLayer(hidden_size)(lstm_stator)\n",
    "    channel_attention_stator = ChannelAttentionLayer(hidden_size * 2)(time_attention_stator)\n",
    "\n",
    "    # Rotor vibration input\n",
    "    input_rotor = Input(shape=rotor_input_shape, name='rotor_input')\n",
    "    lstm_rotor = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_rotor)\n",
    "    time_attention_rotor = TimeAttentionLayer(hidden_size)(lstm_rotor)\n",
    "    channel_attention_rotor = ChannelAttentionLayer(hidden_size * 2)(time_attention_rotor)\n",
    "\n",
    "    # Concatenate and Dense layers with Dropout\n",
    "    concatenated = Concatenate()([channel_attention_stator, channel_attention_rotor])\n",
    "    dense1 = Dense(64, activation='relu')(concatenated)\n",
    "    dropout1 = Dropout(0.6)(dense1)\n",
    "    output = Dense(1)(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_stator, input_rotor], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "# input shape for stator and rotor data\n",
    "stator_input_shape = (100, 6) # (sequence_length, number of stator features)\n",
    "rotor_input_shape = (100, 1) # (sequence_length, number of rotor features)\n",
    "\n",
    "model = create_model(stator_input_shape, rotor_input_shape)\n",
    "model.summary\n",
    "\n",
    "# Fit the model using the data generator\n",
    "# model.fit(generator, steps_per_epoch=(stator_data[0].shape[0] - 100) // 32, epochs=10)\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='bilstm_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling ChannelAttentionLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'channel_attention_layer_3' (of type ChannelAttentionLayer). Either the `ChannelAttentionLayer.call()` method is incorrect, or you need to implement the `ChannelAttentionLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInputs have incompatible shapes. Received shapes (100, 16) and (128,)\u001b[0m\n\nArguments received by ChannelAttentionLayer.call():\n  • args=('<KerasTensor shape=(None, 100, 128), dtype=float32, sparse=None, name=keras_tensor_45>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m time_attention_model \u001b[38;5;241m=\u001b[39m visualize_time_attention((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m128\u001b[39m))  \u001b[38;5;66;03m# Adjust input_shape accordingly\u001b[39;00m\n\u001b[0;32m     25\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(time_attention_model, to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_attention_model.png\u001b[39m\u001b[38;5;124m'\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 27\u001b[0m channel_attention_model \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_channel_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust input_shape accordingly\u001b[39;00m\n\u001b[0;32m     28\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(channel_attention_model, to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_attention_model.png\u001b[39m\u001b[38;5;124m'\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Visualize the entire model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 16\u001b[0m, in \u001b[0;36mvisualize_channel_attention\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_channel_attention\u001b[39m(input_shape):\n\u001b[0;32m     15\u001b[0m     input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m---> 16\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[43mChannelAttentionLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39mattention_output)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[36], line 52\u001b[0m, in \u001b[0;36mChannelAttentionLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     49\u001b[0m W22 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2(m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Combine using element-wise summation and add n\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mAdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mW12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW22\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Activation function to get attention weights\u001b[39;00m\n\u001b[0;32m     55\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(combined)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception encountered when calling ChannelAttentionLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'channel_attention_layer_3' (of type ChannelAttentionLayer). Either the `ChannelAttentionLayer.call()` method is incorrect, or you need to implement the `ChannelAttentionLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nInputs have incompatible shapes. Received shapes (100, 16) and (128,)\u001b[0m\n\nArguments received by ChannelAttentionLayer.call():\n  • args=('<KerasTensor shape=(None, 100, 128), dtype=float32, sparse=None, name=keras_tensor_45>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# Visualization Functions\n",
    "def visualize_bilstm(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=bilstm_layer)\n",
    "    return model\n",
    "\n",
    "def visualize_time_attention(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    attention_output = TimeAttentionLayer()(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=attention_output)\n",
    "    return model\n",
    "\n",
    "def visualize_channel_attention(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    attention_output = ChannelAttentionLayer()(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=attention_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "bilstm_model = visualize_bilstm((100, 6))  # Adjust input_shape accordingly\n",
    "tf.keras.utils.plot_model(bilstm_model, to_file='bilstm_model.png', show_shapes=True)\n",
    "\n",
    "time_attention_model = visualize_time_attention((100, 128))  # Adjust input_shape accordingly\n",
    "tf.keras.utils.plot_model(time_attention_model, to_file='time_attention_model.png', show_shapes=True)\n",
    "\n",
    "channel_attention_model = visualize_channel_attention((100, 128))  # Adjust input_shape accordingly\n",
    "tf.keras.utils.plot_model(channel_attention_model, to_file='channel_attention_model.png', show_shapes=True)\n",
    "\n",
    "# Visualize the entire model\n",
    "tf.keras.utils.plot_model(model, to_file='full_model.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_thesis-python",
   "language": "python",
   "name": "dl_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
