{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threading parameters before importing TensorFlow\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '20'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '20'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Concatenate, Multiply, Permute, Lambda, Reshape, Activation, Dropout, Add, GlobalMaxPooling1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "import pydot\n",
    "\n",
    "# Set threading configuration\n",
    "tf.config.threading.set_intra_op_parallelism_threads(20)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Intra op parallelism threads:  20\n",
      "Inter op parallelism threads:  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Intra op parallelism threads: \", tf.config.threading.get_intra_op_parallelism_threads())\n",
    "print(\"Inter op parallelism threads: \", tf.config.threading.get_inter_op_parallelism_threads())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU and CPU configuration  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "# ensuring Tensorflow is using the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Using the CPU instead.\")\n",
    "\n",
    "# Enable mixed percision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this section only if there exists string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'Time' column\n",
    "def clean_time_column(df):\n",
    "    # Ensure the 'Time' column is treated as a string\n",
    "    df['Time'] = df['Time'].astype(str)\n",
    "    # Remove the unit 'sec' from the 'Time' column\n",
    "    df['Time'] = df['Time'].str.replace(' sec', '', regex=False)\n",
    "    # Convert the 'Time' column to a numeric type\n",
    "    df['Time'] = pd.to_numeric(df['Time'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Function to handle missing values\n",
    "def handle_missing_values(df, method='drop', fill_value=0):\n",
    "    if method == 'drop':\n",
    "        # Drop rows with missing values\n",
    "        df = df.dropna()\n",
    "    elif method == 'fill':\n",
    "        # Fill missing values with a specified value\n",
    "        df = df.fillna(fill_value)\n",
    "    elif method == 'interpolate':\n",
    "        # Interpolate missing values\n",
    "        df = df.interpolate()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method for handling missing values. Use 'drop', 'fill', or 'interpolate'.\")\n",
    "    return df\n",
    "\n",
    "# Function to clean all files in a given list and save them to a directory\n",
    "def clean_csv_files(files, save_directory, missing_value_method='fill', fill_value=0):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)  # Create directory if it doesn't exist\n",
    "    \n",
    "    for file in files:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Check if 'Time' column exists\n",
    "        if 'Time' in df.columns:\n",
    "            # Clean the 'Time' column\n",
    "            df = clean_time_column(df)\n",
    "            # Handle missing values\n",
    "            df = handle_missing_values(df, method=missing_value_method, fill_value=fill_value)\n",
    "            # Define the new file path with the same name but in the save directory\n",
    "            filename = os.path.basename(file)\n",
    "            cleaned_file_path = os.path.join(save_directory, filename)\n",
    "            # Save the cleaned file\n",
    "            df.to_csv(cleaned_file_path, index=False)\n",
    "            print(f\"Cleaned file saved: {cleaned_file_path}\")\n",
    "        else:\n",
    "            print(f\"No 'Time' column found in file: {file}\")\n",
    "\n",
    "# Directory where you want to save the cleaned files\n",
    "save_directory = r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\"\n",
    "\n",
    "# List all of the CSV file paths\n",
    "stator_csv_files_str = [r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\01_SCIM_current_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\02_SCIM_current_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\03_SCIM_current_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\04_SCIM_current_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\05_SCIM_current_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\06_SCIM_current_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\07_SCIM_current_PTPbc_400V.csv\"]\n",
    "\n",
    "rotor_csv_files_str = [r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\01_SCIM_vibration_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\02_SCIM_vibration_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\03_SCIM_vibration_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\04_SCIM_vibration_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\05_SCIM_vibration_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\06_SCIM_vibration_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data-with_string\\07_SCIM_vibration_PTPbc_400V.csv\"]\n",
    "\n",
    "# Clean the stator and rotor CSV files and save them to the specified directory\n",
    "print(\"Cleaning stator files...\")\n",
    "clean_csv_files(stator_csv_files_str, save_directory)\n",
    "print(\"\\nCleaning rotor files...\")\n",
    "clean_csv_files(rotor_csv_files_str, save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all of the CSV file paths\n",
    "stator_csv_files = [r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\01_SCIM_current_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\02_SCIM_current_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\03_SCIM_current_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\04_SCIM_current_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\05_SCIM_current_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\06_SCIM_current_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\07_SCIM_current_PTPbc_400V.csv\"]\n",
    "\n",
    "rotor_csv_files = [r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\01_SCIM_vibration_allfault_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\02_SCIM_vibration_PTPab_400V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\03_SCIM_vibration_PTPab_480V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\04_SCIM_vibration_PTGa_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\05_SCIM_vibration_PTGb_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\06_SCIM_vibration_PTGc_360V.csv\",\n",
    "r\"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\\07_SCIM_vibration_PTPbc_400V.csv\"]\n",
    "\n",
    "# Read the CSV files into pandas dataframe and convert to numpy arrays\n",
    "stator_data = [pd.read_csv(file).values for file in stator_csv_files]\n",
    "rotor_data = [pd.read_csv(file).values for file in rotor_csv_files]\n",
    "\n",
    "# Split the data into taining and validation set\n",
    "train_size = int(len(stator_data[0]) * 0.8) # 80% training data and 20% validation data\n",
    "\n",
    "train_stator_data = [data[:train_size] for data in stator_data]\n",
    "val_stator_data = [data[train_size:] for data in stator_data]\n",
    "\n",
    "train_rotor_data = [data[:train_size] for data in rotor_data]\n",
    "val_rotor_data = [data[train_size:] for data in rotor_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std for normalization\n",
    "def compute_mean_std(data):\n",
    "    concatenated_data = np.concatenate(data, axis= 0)\n",
    "    mean = np.mean(concatenated_data, axis= 0)\n",
    "    std = np.std(concatenated_data, axis= 0)\n",
    "    return mean, std\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_data(data, mean, std):\n",
    "    return [(d - mean) / std for d in data]\n",
    "\n",
    "# Compute mean and std for stator and rotor data\n",
    "stator_mean, stator_std = compute_mean_std(train_stator_data)\n",
    "rotor_mean, rotor_std = compute_mean_std(train_rotor_data) \n",
    "\n",
    "# Normalize the training data \n",
    "stator_train_data_normalized = normalize_data(train_stator_data, stator_mean, stator_std)\n",
    "rotor_train_data_normalized = normalize_data(train_rotor_data, rotor_mean, rotor_std)\n",
    "\n",
    "# Normalize the validation data using the training data statistics\n",
    "stator_val_data_normalized = normalize_data(val_stator_data, stator_mean, stator_std)\n",
    "rotor_val_data_normalized = normalize_data(val_rotor_data, rotor_mean, rotor_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(stator_data, rotor_data, sequence_length=100, batch_size=32):\n",
    "    # get the number of samples\n",
    "    num_samples = stator_data[0].shape[0] - sequence_length     \n",
    "\n",
    "    # Infinite loop to continuously yield the batches\n",
    "    while True:\n",
    "        # Generate batches\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            stator_batch = []\n",
    "            rotor_batch = []\n",
    "            batch_y = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                end_idx = start_idx + i + sequence_length\n",
    "                if end_idx >= num_samples:\n",
    "                    break\n",
    "                # Create sequences of length 'Sequence length'\n",
    "                stator_seq = [data[start_idx + i:end_idx, 1:] for data in stator_data] # excluding the time column\n",
    "                rotor_seq = [data[start_idx + i:end_idx, 1] for data in rotor_data] # excluding the time column & vibration is the only remaining column\n",
    "            \n",
    "                # Target is the next time step's 'Time' value in stator data (same as rotor data)\n",
    "                y = stator_data[0][end_idx, 0] # assuming time is the first column in the stator files\n",
    "\n",
    "                stator_batch.append(np.concatenate(stator_seq, axis=-1))\n",
    "                rotor_batch.append(np.concatenate(rotor_seq, axis=-1))\n",
    "                batch_y.append(y)\n",
    "        if len(stator_batch) == batch_size:\n",
    "            X_stator = np.array(stator_batch)\n",
    "            X_rotor = np.array(rotor_batch)\n",
    "            y = np.array(batch_y)\n",
    "\n",
    "            #concatenating data along the last axis\n",
    "            yield [X_stator, X_rotor], y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generator for both training and validation data with the normalized data\n",
    "\n",
    "train_generator = data_generator(stator_train_data_normalized, rotor_train_data_normalized, sequence_length=100, batch_size=32)\n",
    "validation_generator = data_generator(stator_val_data_normalized, rotor_val_data_normalized, sequence_length=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BiLSTM network with Time and Channel attention\n",
    "\n",
    "### Defining Time attention and Channel attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time attention mechanism \n",
    "class TimeAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, **kwargs):\n",
    "        super(TimeAttentionLayer, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_omega = Dense(hidden_size, use_bias=False)\n",
    "        self.b_omega = Dense(hidden_size, use_bias=False)\n",
    "        self.u_omega = self.add_weight(shape=(hidden_size,), initializer='random_normal', trainable=True, name='u_omega')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_t = tf.nn.tanh(self.W_omega(inputs) + self.b_omega(inputs))\n",
    "        alpha_t = tf.nn.softmax(tf.reduce_sum(u_t * self.u_omega, axis=2, keepdims=True), axis=1)\n",
    "        context_vector = tf.reduce_sum(alpha_t * inputs, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "class ChannelAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ChannelAttentionLayer, self).__init__(**kwargs)\n",
    "        self.input_shape_param = input_shape\n",
    "        self.gmp = GlobalMaxPooling1D()\n",
    "        self.gap = GlobalAvgPool1D()\n",
    "        self.fcl = Dense(input_shape, activation='linear')\n",
    "        self.m = Dense(input_shape // 8, activation='relu')\n",
    "        self.v = Dense(input_shape // 8, activation='relu')\n",
    "        self.n = Dense(input_shape // 8, activation='relu')\n",
    "        self.W11 = Dense(input_shape // 8, activation='relu')\n",
    "        self.W12 = Dense(input_shape, activation='linear')\n",
    "        self.W21 = Dense(input_shape // 8, activation='relu')\n",
    "        self.W22 = Dense(input_shape, activation='linear')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ChannelAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, sequence_length, feature_dim = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]\n",
    "\n",
    "        # GMP and GAP for descriptors m and v\n",
    "        gmp = self.gmp(inputs)\n",
    "        gap = self.gap(inputs)\n",
    "        \n",
    "        # Fully Connected Layer for descriptor n\n",
    "        fcl = self.fcl(inputs)\n",
    "\n",
    "        # Descriptors\n",
    "        m = self.m(gmp)\n",
    "        v = self.v(gap)\n",
    "        n = self.n(fcl)\n",
    "\n",
    "        # Multilayer Perceptrons\n",
    "        W11 = self.W11(v)\n",
    "        W12 = self.W12(W11)\n",
    "\n",
    "        W21 = self.W21(m)\n",
    "        W22 = self.W22(W21)\n",
    "\n",
    "        # reshape the descriptors to match before adding (newly updated on 11.07.2024 after errors)\n",
    "        W12_reshaped = tf.reshape(W12, [-1, feature_dim])\n",
    "        W22_reshaped = tf.reshape(W22, [-1, feature_dim])\n",
    "        n_reshaped = tf.reshape(n, [-1, feature_dim])\n",
    "\n",
    "        # Combine using element-wise summation and add n\n",
    "        combined = Add()([W12_reshaped, W22_reshaped, n_reshaped])\n",
    "\n",
    "        # Activation function to get attention weights\n",
    "        attention_weights = Activation('hard_sigmoid')(combined)\n",
    "\n",
    "        # Tile attention weights to match the input shape (newly updated on 11.07.2024 after errors)\n",
    "        attention_weights = tf.reshape(attention_weights, [batch_size, 1, feature_dim])\n",
    "        attention_weights = tf.tile(attention_weights, [1, sequence_length, 1])\n",
    "\n",
    "        # Apply attention weights\n",
    "        channel_attention = Multiply()([inputs, attention_weights])\n",
    "        return channel_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model with Keras-tuner Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\mca_bilstm_tuning\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 3\n",
      "hidden_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.6, 'step': 0.2, 'sampling': 'linear'}\n",
      "dense_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "32                |96                |hidden_size\n",
      "0.4               |0.5               |dropout_rate\n",
      "96                |128               |dense_size\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Display the search space summary\u001b[39;00m\n\u001b[0;32m     47\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()\n\u001b[1;32m---> 49\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_stator_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_stator_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m             \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# function to check if the graphviz package is working properly\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_graphviz\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\training.py:1501\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1492\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1493\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[0;32m   1494\u001b[0m         )\n\u001b[0;32m   1495\u001b[0m     )\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1499\u001b[0m ):\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1501\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1518\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\data_adapter.py:869\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Since we have to know the dtype of the python generator when we build\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# the dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m peek, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_peek_and_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(peek)\n\u001b[0;32m    871\u001b[0m peek \u001b[38;5;241m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\data_adapter.py:935\u001b[0m, in \u001b[0;36mGeneratorDataAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> 935\u001b[0m     peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m peek, itertools\u001b[38;5;241m.\u001b[39mchain([peek], x)\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mdata_generator\u001b[1;34m(stator_data, rotor_data, sequence_length, batch_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m         y \u001b[38;5;241m=\u001b[39m stator_data[\u001b[38;5;241m0\u001b[39m][end_idx, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# assuming time is the first column in the stator files\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         stator_batch\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate(stator_seq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m         rotor_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotor_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m         batch_y\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stator_batch) \u001b[38;5;241m==\u001b[39m batch_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input shape for stator and rotor data\n",
    "stator_input_shape = (100, 6) # (sequence_length, number of stator features)\n",
    "rotor_input_shape = (100, 1) # (sequence_length, number of rotor features)\n",
    "\n",
    "# Define the model with modified channel attention mechanism and dropout\n",
    "def build_model(hp):\n",
    "    hidden_size = hp.Int('hidden_size', min_value=32, max_value=128, step=32)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.6, step=0.2)\n",
    "    dense_size = hp.Int('dense_size', min_value=32, max_value=128, step=32)\n",
    "\n",
    "    # Stator current input\n",
    "    input_stator = Input(shape=stator_input_shape, name='stator_input')\n",
    "    lstm_stator = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_stator)\n",
    "    time_attention_stator = TimeAttentionLayer(hidden_size)(lstm_stator)\n",
    "\n",
    "    # Rotor vibration input\n",
    "    input_rotor = Input(shape=rotor_input_shape, name='rotor_input')    \n",
    "    lstm_rotor = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_rotor)\n",
    "    time_attention_rotor = TimeAttentionLayer(hidden_size)(lstm_rotor)\n",
    "\n",
    "    # Concatenate and Dense layers with Dropout\n",
    "    concatenated = Concatenate()([time_attention_stator, time_attention_rotor])\n",
    "    concatenated = tf.expand_dims(concatenated, axis=1)\n",
    "    channel_attention_layer = ChannelAttentionLayer(hidden_size * 2)(concatenated)\n",
    "    dense1 = Dense(dense_size, activation='relu')(tf.squeeze(channel_attention_layer, axis=1))\n",
    "    dropout1 = Dropout(dropout_rate)(dense1)\n",
    "    output = Dense(1)(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_stator, input_rotor], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Setup the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    directory='my_dir',\n",
    "    project_name='mca_bilstm_tuning'\n",
    ")\n",
    "\n",
    "# Display the search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(train_generator, steps_per_epoch=(len(train_stator_data[0]) - 100) // 32, epochs=2, \n",
    "             validation_data=validation_generator, validation_steps=(len(val_stator_data[0]) - 100) // 32, \n",
    "             use_multiprocessing=True, callbacks=[early_stopping, tensorboard_callback])\n",
    "\n",
    "# function to check if the graphviz package is working properly\n",
    "def check_graphviz():\n",
    "    try:\n",
    "        # Attempt to create an image of a blank graph to check the pydot/graphviz installation.\n",
    "        pydot.Dot.create(pydot.Dot())\n",
    "        return True\n",
    "    except (OSError, Exception) as e:\n",
    "        if isinstance(e, OSError):\n",
    "            print(\"Graphviz not found.\")\n",
    "        else:\n",
    "            print(\"Some other exception occurred with pydot.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the Best model obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hidden size: {best_hps.get('hidden_size')}\")\n",
    "print(f\"Best dropout rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"Best dense rate: {best_hps.get('dense_size')}\")\n",
    "\n",
    "# Build the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "# best_model.fit(train_generator, steps_per_epoch=(len(train_stator_data[0]) - 100) // 32, epochs=10,\n",
    "               # validation_data=validation_generator, validation_steps=(len(val_stator_data[0]) - 100 // 32)\n",
    "\n",
    "# Evaluate the best model\n",
    "# predictions = best_model.predict(validation_generator)\n",
    "# true_y = np.concatenate([y for _, y in validation_generator], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if Graphviz is installed correctly\n",
    "# if check_graphviz():\n",
    "    # Save the model architecture to a file\n",
    "    # plot_model(model, to_file='bilstm_model.png', show_shapes=True)\n",
    "# else:\n",
    "    # print(\"Graphviz is not installed or not in PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_bilstm(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=bilstm_layer)\n",
    "    return model\n",
    "\n",
    "def visualize_time_attention(input_shape, hidden_size):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    attention_output = TimeAttentionLayer(hidden_size)(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=attention_output)\n",
    "    return model\n",
    "\n",
    "def visualize_channel_attention(input_shape, hidden_size):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    attention_output = ChannelAttentionLayer(hidden_size)(input_layer)\n",
    "    model = Model(inputs=input_layer, outputs=attention_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "bilstm_model = visualize_bilstm((100, 6))  # Adjust input_shape accordingly\n",
    "plot_model(bilstm_model, to_file='bilstm_model.png', show_shapes=True)\n",
    "\n",
    "time_attention_model = visualize_time_attention((100, 128), hidden_size=128)  # Adjust input_shape accordingly\n",
    "plot_model(time_attention_model, to_file='time_attention_model.png', show_shapes=True)\n",
    "\n",
    "channel_attention_model = visualize_channel_attention((100, 128), hidden_size=128)  # Adjust input_shape accordingly\n",
    "plot_model(channel_attention_model, to_file='channel_attention_model.png', show_shapes=True)\n",
    "\n",
    "# Build the full model for visualization\n",
    "hp = kt.HyperParameters()\n",
    "model = build_model(hp)\n",
    "# Visualize the entire model\n",
    "plot_model(model, to_file='full_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Model accuracy using MSE, RMSE, MAE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming `predictions` and `true_y` are already defined as per the previous code\n",
    "mse = mean_squared_error(true_y, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_y, predictions)\n",
    "r2 = r2_score(true_y, predictions)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'R-squared (R²): {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_thesis_309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
