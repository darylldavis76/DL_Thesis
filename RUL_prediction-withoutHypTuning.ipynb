{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Concatenate, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "csv_directory = \"D:\\MTdataset\\DL_dataset\\Python_DL\\Final_current_vibration_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and combine all the current and vibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the current and vibration data\n",
    "current_files = [file for file in os.listdir(csv_directory) if 'current' in file]\n",
    "vibration_files = [file for file in os.listdir(csv_directory) if 'vibration' in file]\n",
    "\n",
    "current_dfs = [pd.read_csv(os.path.join(csv_directory, file)) for file in current_files]\n",
    "vibration_dfs = [pd.read_csv(os.path.join(csv_directory, file)) for file in vibration_files]\n",
    "\n",
    "# Combine all current data and vibration data into single DataFrames\n",
    "current_data = pd.concat(current_dfs, ignore_index=True)\n",
    "vibration_data = pd.concat(vibration_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Take care of missing values and data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no missing values\n",
    "current_data = current_data.dropna()\n",
    "vibration_data = vibration_data.dropna()\n",
    "\n",
    "# Normalize the data (excluding the Time column)\n",
    "scaler_current = MinMaxScaler()\n",
    "scaler_vibration = MinMaxScaler()\n",
    "\n",
    "current_data.iloc[:, 1:] = scaler_current.fit_transform(current_data.iloc[:, 1:])\n",
    "vibration_data.iloc[:, 1:] = scaler_vibration.fit_transform(vibration_data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation - creating data sequences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i + sequence_length].values\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "current_sequences = create_sequences(current_data, sequence_length)\n",
    "vibration_sequences = create_sequences(vibration_data, sequence_length)\n",
    "\n",
    "# Assume the RUL is the last value in the Time column for each sequence\n",
    "rul = current_data['Time'][sequence_length:].values\n",
    "\n",
    "# Train-validation split\n",
    "train_current_seq, val_current_seq, train_vibration_seq, val_vibration_seq, train_rul, val_rul = train_test_split(\n",
    "    current_sequences, vibration_sequences, rul, test_size=0.2, random_state=42)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, current_sequences, vibration_sequences, rul, batch_size):\n",
    "        self.current_sequences = current_sequences\n",
    "        self.vibration_sequences = vibration_sequences\n",
    "        self.rul = rul\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.current_sequences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_current = self.current_sequences[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_vibration = self.vibration_sequences[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_rul = self.rul[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return [batch_current, batch_vibration], batch_rul\n",
    "\n",
    "# Create generators\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(train_current_seq, train_vibration_seq, train_rul, batch_size)\n",
    "validation_generator = DataGenerator(val_current_seq, val_vibration_seq, val_rul, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Channel Attention layer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layers for time and channel attention\n",
    "class TimeAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TimeAttentionLayer, self).__init__()\n",
    "        self.W_h = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W_t = tf.keras.layers.Dense(hidden_size)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h_t = inputs\n",
    "        u_t = tf.nn.tanh(self.W_h(h_t) + tf.expand_dims(self.W_t(h_t[:, -1, :]), axis=1))\n",
    "        a_t = tf.nn.softmax(self.v(u_t), axis=1)\n",
    "        S_t = tf.reduce_sum(a_t * h_t, axis=1)\n",
    "        return S_t\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TimeAttentionLayer, self).get_config()\n",
    "        config.update({\"hidden_size\": self.hidden_size})\n",
    "        return config\n",
    "\n",
    "class ChannelAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.W_v = tf.keras.layers.Dense(num_channels)\n",
    "        self.W_m = tf.keras.layers.Dense(num_channels)\n",
    "        self.W_n = tf.keras.layers.Dense(num_channels)\n",
    "        self.W_a = tf.keras.layers.Dense(num_channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        H = tf.expand_dims(inputs, axis=1)  # Add an extra dimension\n",
    "        v_j = GlobalAveragePooling1D()(H)\n",
    "        m_j = GlobalMaxPooling1D()(H)\n",
    "        n_j = tf.reduce_sum(H, axis=1)\n",
    "        concatenated = tf.concat([v_j, m_j, n_j], axis=1)\n",
    "        r = tf.nn.relu(self.W_v(concatenated) + self.W_m(concatenated) + self.W_n(concatenated))\n",
    "        attention_weights = self.W_a(r)\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=1)\n",
    "        attention_weights = tf.tile(attention_weights, [1, 1, tf.shape(inputs)[1]])\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[0, 2, 1])\n",
    "        attention_output = attention_weights * H\n",
    "        return tf.reduce_sum(attention_output, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ChannelAttentionLayer, self).get_config()\n",
    "        config.update({\"num_channels\": self.num_channels})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCA BiLSTM model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shapes\n",
    "stator_input_shape = (100, 6)  # (sequence_length, number of stator features)\n",
    "rotor_input_shape = (100, 1)  # (sequence_length, number of rotor features)\n",
    "\n",
    "# Define the model\n",
    "def build_mca_bilstm_model(hidden_size, dropout_rate, dense_size):\n",
    "    # Stator current input\n",
    "    input_stator = Input(shape=stator_input_shape, name='stator_input')\n",
    "    lstm_stator = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_stator)\n",
    "    time_attention_stator = TimeAttentionLayer(hidden_size)(lstm_stator)\n",
    "    \n",
    "    print(\"Shape after time attention (stator):\", time_attention_stator.shape)\n",
    "\n",
    "    # Rotor vibration input\n",
    "    input_rotor = Input(shape=rotor_input_shape, name='rotor_input')\n",
    "    lstm_rotor = Bidirectional(LSTM(hidden_size, return_sequences=True))(input_rotor)\n",
    "    time_attention_rotor = TimeAttentionLayer(hidden_size)(lstm_rotor)\n",
    "    \n",
    "    print(\"Shape after time attention (rotor):\", time_attention_rotor.shape)\n",
    "\n",
    "    # Concatenate outputs from time attention layers\n",
    "    concatenated = Concatenate()([time_attention_stator, time_attention_rotor])\n",
    "    print(\"Shape after concatenation:\", concatenated.shape)\n",
    "\n",
    "    # Channel attention layer\n",
    "    channel_attention_layer = ChannelAttentionLayer(hidden_size * 2)(concatenated)\n",
    "    print(\"Shape after channel attention:\", channel_attention_layer.shape)\n",
    "\n",
    "    # Dense layers with Dropout\n",
    "    dense1 = Dense(dense_size, activation='relu')(channel_attention_layer)\n",
    "    dropout1 = Dropout(dropout_rate)(dense1)\n",
    "    output = Dense(1, name='RUL')(dropout1)\n",
    "\n",
    "    model = Model(inputs=[input_stator, input_rotor], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after time attention (stator): (None, 128)\n",
      "Shape after time attention (rotor): (None, 128)\n",
      "Shape after concatenation: (None, 256)\n",
      "Shape after channel attention: (None, 256)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " stator_input (InputLayer)      [(None, 100, 6)]     0           []                               \n",
      "                                                                                                  \n",
      " rotor_input (InputLayer)       [(None, 100, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_18 (Bidirectiona  (None, 100, 128)    36352       ['stator_input[0][0]']           \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional_19 (Bidirectiona  (None, 100, 128)    33792       ['rotor_input[0][0]']            \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " time_attention_layer_18 (TimeA  (None, 128)         16577       ['bidirectional_18[0][0]']       \n",
      " ttentionLayer)                                                                                   \n",
      "                                                                                                  \n",
      " time_attention_layer_19 (TimeA  (None, 128)         16577       ['bidirectional_19[0][0]']       \n",
      " ttentionLayer)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 256)          0           ['time_attention_layer_18[0][0]',\n",
      "                                                                  'time_attention_layer_19[0][0]']\n",
      "                                                                                                  \n",
      " channel_attention_layer_9 (Cha  (None, 256)         311808      ['concatenate_9[0][0]']          \n",
      " nnelAttentionLayer)                                                                              \n",
      "                                                                                                  \n",
      " dense_99 (Dense)               (None, 128)          32896       ['channel_attention_layer_9[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_99[0][0]']               \n",
      "                                                                                                  \n",
      " RUL (Dense)                    (None, 1)            129         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 448,131\n",
      "Trainable params: 448,131\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "52498/52498 [==============================] - 4377s 83ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/5\n",
      "52498/52498 [==============================] - 4812s 92ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/5\n",
      "52498/52498 [==============================] - 5029s 96ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/5\n",
      "52498/52498 [==============================] - 4530s 86ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/5\n",
      "52498/52498 [==============================] - 4493s 86ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20860958be0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 64\n",
    "dropout_rate = 0.3\n",
    "dense_size = 128\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mca_bilstm_model(hidden_size, dropout_rate, dense_size)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=5, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([val_current_seq, val_vibration_seq])\n",
    "mse = mean_squared_error(val_rul, predictions)\n",
    "print('Mean squared error:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "\nLayer TimeAttentionLayer has arguments ['hidden_size']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmca_bilstm_model_minmax.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the scalers\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\daryl\\anaconda3\\envs\\DL_thesis_309\\lib\\site-packages\\keras\\engine\\base_layer.py:786\u001b[0m, in \u001b[0;36mLayer.get_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;66;03m# Check that either the only argument in the `__init__` is  `self`,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;66;03m# or that `get_config` has been overridden:\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_default\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    787\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    788\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124m  Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_args\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;124m  in `__init__` and therefore must override `get_config()`.\u001b[39m\n\u001b[0;32m    791\u001b[0m \n\u001b[0;32m    792\u001b[0m \u001b[38;5;124m  Example:\u001b[39m\n\u001b[0;32m    793\u001b[0m \n\u001b[0;32m    794\u001b[0m \u001b[38;5;124m  class CustomLayer(keras.layers.Layer):\u001b[39m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;124m      def __init__(self, arg1, arg2):\u001b[39m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124m          super().__init__()\u001b[39m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124m          self.arg1 = arg1\u001b[39m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;124m          self.arg2 = arg2\u001b[39m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[38;5;124m      def get_config(self):\u001b[39m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124m          config = super().get_config()\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124m          config.update(\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg1,\u001b[39m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: self.arg2,\u001b[39m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124m          \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124m          return config\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    807\u001b[0m         )\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: \nLayer TimeAttentionLayer has arguments ['hidden_size']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('mca_bilstm_model_minmax.h5')\n",
    "\n",
    "# Save the scalers\n",
    "import joblib\n",
    "joblib.dump(scaler_current, 'scaler_current_minmax.pkl')\n",
    "joblib.dump(scaler_vibration, 'scaler_vibration_minmax.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_thesis_309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
